{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Review of cluster architecture\n",
    "\n",
    "(diagram goes here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Review of Slurm allocations\n",
    "\n",
    "$ srun [options] batch.sh\n",
    "\n",
    "option | example | comment\n",
    "-------|--------|---\n",
    "-c _cores_ | -c 20 | cores/task on single node\n",
    "-n _tasks_ | -n 4 | mpi progs only\n",
    "-N _nodes_ | -N 10 | never useful\n",
    "-t _time_  | -t 7-, -t 3:00 | job killed if exceeded\n",
    "--mem=_mem_ | --mem=16g | ditto\n",
    "--mem-per-cpu=_mem_ | --mem-per-cpu=8g | ditto\n",
    "-J _name_ | -J myjob | for user only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking about memory requirements\n",
    "- default 5GB/core on our clusters\n",
    "- strictly enforced; jobs exceeding limit killed\n",
    "- you can request custom memory per node or core\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining memory requirements\n",
    "During run:\n",
    "- ssh to node, run \n",
    "    \"top -o RES -u _netid_\"\n",
    "    look at RES column\n",
    "- /usr/bin/time -a _prog args_\n",
    "\n",
    "After successful run, determine actual usage:\n",
    "\n",
    "- sacct\n",
    "- remora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slurm sacct\n",
    "\n",
    "sacct -o 'JobID,MaxRSS,MaxVMSize' -j _jobid_\n",
    "\n",
    "or \n",
    "\n",
    "Configure sacct format:\n",
    "\n",
    " export SACCT_FORMAT=JobID%-20,JobName,User,Partition,NodeList,Elapsed,State,ExitCode,MaxRSS, AllocTRES%32\n",
    " \n",
    " sacct -j _jobid_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remora\n",
    "https://github.com/TACC/remora\n",
    "\n",
    "module load REMORA\n",
    "remora prog args ...\n",
    "\n",
    "This will create a directory: remora_jobid\n",
    "\n",
    "Copy (rsync) to local computer, open remora_summary.html with browser \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Compute Resources\n",
    "\n",
    "To get overall sense:\n",
    "```\n",
    "sinfo -p general\n",
    "```\n",
    "To see completely idle nodes, by core count:\n",
    "\n",
    "```\n",
    "$ sinfo -p general -e -t IDLE -o \"%P %.5a %c %.10l %.6D %.6t %N\"\n",
    "PARTITION AVAIL CPUS  TIMELIMIT  NODES  STATE NODELIST\n",
    "general*    up 8 30-00:00:0     35   idle c06n[10-16],c07n[01-14,16],c08n[01-06,08-14]\n",
    "general*    up 16 30-00:00:0     23   idle c10n[13-16],c11n[01-16],c12n[09-11]\n",
    "```\n",
    "\n",
    "Hint, use alias:\n",
    "```\n",
    "alias findidle='sinfo -p general -e -t IDLE -o \"%P %.5a %c %.10l %.6D %.6t %N\"'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairshare scheduling\n",
    "- Groups and users with heavy recent usage (last 30-45 days) have lower priority\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scavenge Partition\n",
    "- Compute nodes in other partions are available via scavenge partition\n",
    "- sbatch -p scavenge ...\n",
    "- separate per user limits apply\n",
    "- works best for short jobs, dSQ/array jobs, or jobs that checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Memory Nodes\n",
    "- We have some compute nodes with 512GB-1.5TB of RAM\n",
    "- Reserved for applications with large memory needs. Please be considerate.\n",
    "- Separate slurm partition: bigmem\n",
    "\n",
    "Typical allocation: \n",
    "```\n",
    "srun/sbatch -p bigmem --mem=1500g ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Nodes\n",
    "- Some applications have been ported to GPUs with impressive performance improvement\n",
    "- Gpu nodes have conventional cpus with multiple cores, and 1-4 GPUs.  \n",
    "- To use GPUs, you must:\n",
    " - request node(s) with GPUs\n",
    " - request the type and number of GPUs \n",
    "\n",
    "Typical allocation:\n",
    "```\n",
    "srun/sbatch -p gpu -c 20 --gres=gpu:1080ti:4 ...\n",
    "```\n",
    "Note that partition names, types and number of GPUs vary by cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism\n",
    "\n",
    "- Sbatch can allocate multiple cores and nodes, but the script runs on one core on one node sequentially.\n",
    "\n",
    "- Simply allocating more nodes or cores DOES NOT make jobs faster.\n",
    "\n",
    "- How do we use multiple cores to increase speed?\n",
    "\n",
    "\n",
    "- Two classes of parallelism:\n",
    " - Lots of independent sequential jobs\n",
    " - Single job parallelized (somehow)\n",
    " \n",
    "\n",
    "- Some options:\n",
    " - Submit many batch jobs simultaneously (not good)\n",
    " - Use job arrays, or dSQ (much better)\n",
    " - Submit a parallel version of your program (great if you have one)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Arrays\n",
    "\n",
    "- Useful when you have many nearly identical, independent jobs to run\n",
    "- Starts many identical copies of your script, distinguished by a task id.\n",
    "\n",
    "Submit jobs like this:\n",
    "```\n",
    "sbatch --array=1-100 ...\n",
    "```\n",
    "Inside your batch script this environment variable to do something different in each task:\n",
    "```\n",
    "./mycommand -i input.${SLURM_ARRAY_TASK_ID} \\\n",
    "    -o output.${SLURM_ARRAY_TASK_ID}\n",
    "```\n",
    "\n",
    "A few nice features of job arrays:\n",
    "- only one job to keep track of\n",
    "- easy to start or cancel entire set\n",
    "- time limits apply to each task, not overall job\n",
    "- your allocation can grow and shrink as conditions change\n",
    "- when using scavenge partition, tasks are killed, but job persists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dSQ (aka Dead Simple Queue)\n",
    "- built on job arrays.  Same nice features, but easier to use\n",
    "- more flexible; tasks can be different from one another\n",
    "- reporting and error recovery built in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using dSQ\n",
    "\n",
    "\n",
    "- Create file containing list of commands to run (jobs.txt)\n",
    "```\n",
    "prog arg1 arg2 -o job1.out\n",
    "prog arg1 arg2 -o job2.out\n",
    "...\n",
    "```\n",
    "- Create launch script\n",
    "```\n",
    "module load dSQ\n",
    "dSQ --taskfile jobs.txt [slurm args] > run.sh\n",
    "```\n",
    "\n",
    "slurm args can specify partion, timelimit, memory, etc. in the usual way.\n",
    "\n",
    "- Submit launch script\n",
    "```\n",
    "sbatch run.sh\n",
    "```\n",
    "\n",
    "For more info, see <http://research.computing.yale.edu/support/hpc/user-guide/dead-simple-queue>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dSQ Reporting\n",
    "- When dSQ job is finished, you'll see a file `job_<jobid>_status.tsv`\n",
    "- Generate report:\n",
    "```\n",
    "$ dSQAutopsy jobs.txt job_<jobid>_status.tsv > failedjobs.txt\n",
    "Autopsy Task Report:\n",
    "9 succeeded\n",
    "1 failed\n",
    "0 didn't run.\n",
    "```\n",
    "\n",
    "- If any jobs failed, failedjobs.txt will contain those jobs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some ways to run in parallel\n",
    "- R: multicore\n",
    "- Python: multiprocessing\n",
    "- C: threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Multicore\n",
    "\n",
    "Many R packages have parallelism built in: e.g. bootstrapping (boot)\n",
    "\n",
    "```\n",
    "cores=Sys.getenv(\"SLURM_CPUS_ON_NODE\")\n",
    "boot(data=trees, statistic=volume_estimate, R=50000, parallel=\"multicore\", ncpus=cores)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
