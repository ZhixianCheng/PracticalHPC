{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Practical HPC</center>\n",
    "<p>\n",
    "## <center>Robert Bjornson</center> \n",
    "<p>\n",
    "## <center><i>Yale Center for Research Computing</i></center>\n",
    "<p>\n",
    "## <center>March 2018</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Yale Center for Research Computing?\n",
    "\n",
    "\n",
    "- Independent center under the Provost's office\n",
    "- Created to support your research computing needs\n",
    "- Focus is on high performance computing and storage\n",
    "- ~15 staff, including applications specialists and system engineers\n",
    "- Available to consult with and educate users\n",
    "- Manage compute clusters and support users\n",
    "- Located at 160 St. Ronan st, at the corner of Edwards and St. Ronan\n",
    "- http://research.computing.yale.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Understanding the HPC environment\n",
    "- Understanding your programs behavior\n",
    "- Getting the right answer\n",
    "- Getting your answer faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "- Have logged in to cluster and run simple jobs\n",
    "- understand basics of Slurm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is HPC?\n",
    "- running jobs remotely \n",
    "- using more resources than you have locally\n",
    "- access to large shared resources\n",
    "- running in parallel, or many sequential jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Cluster\n",
    "\n",
    "![alt text](cluster.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Cluster\n",
    "\n",
    "![alt text](yalecluster.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Cluster (Yale, or elsewhere)\n",
    "- Several hundred nodes\n",
    "- Each node has 10s of cpus -> Thousands of total cpus\n",
    "- Each node has hundreds of GB RAM\n",
    "- Multiple PB of shared storage\n",
    "- Fast network connecting nodes and networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Resources\n",
    "- Nodes and CPUs\n",
    "- Node Memory\n",
    "- Job Runtime\n",
    "- Disk Storage\n",
    "- GPUs\n",
    "- etc\n",
    "\n",
    "Nodes, cpus, memory, runtime, GPUs are requested via Slurm options, and enforced by OS.\n",
    "\n",
    "Storage limits enforced by filesystem quotas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Review of Slurm allocations\n",
    "\n",
    "$ sbatch [options] batch.sh\n",
    "\n",
    "$ srun --pty [options] bash\n",
    "\n",
    "option | example | comment\n",
    "-------|--------|---\n",
    "-p _partition_ | -p general | partitions(s) to run on\n",
    "-c _cores_ | -c 20 | cores/task on single node\n",
    "-n _tasks_ | -n 4 | mpi progs only\n",
    "-N _nodes_ | -N 10 | \"never\" useful\n",
    "-t _time_  | -t 7-, -t 3:00 | job killed if exceeded\n",
    "--mem=_mem_ | --mem=16g | ditto\n",
    "--mem-per-cpu=_mem_ | --mem-per-cpu=8g | ditto\n",
    "-J _name_ | -J myjob | name job\n",
    "--gres | --gres=gpu:p100:2 | request gpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is \"Running Efficiently\"?\n",
    "- All allocated cores are running near 100%\n",
    "- memory requested and used are commensurate\n",
    "- File and Network I/O are \"reasonable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical overall user limits (vary by cluster)\n",
    "- cpus (few hundred)\n",
    "- memory (5 GB * cpus)\n",
    "- storage\n",
    " - home: hundreds GB, 500k files\n",
    " - project: few TB (shared among group), 5M files\n",
    " - scratch: many TB (shared among group), 5M files\n",
    " - file counts matter!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage Quotas\n",
    "- varies somewhat by cluster, see cluster doc\n",
    "- examples:\n",
    " - (farnam) /ysm-gpfs/bin/my_quota.sh, group_quota.sh\n",
    " - (ruddle) /ycga-gpfs/bin/my_quota.sh, group_quota.sh\n",
    " - (grace) /gpfs/apps/bin/groupquota.sh \n",
    " - (omeaga-next?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes/CPUs\n",
    "- only request multiple cpus and/or nodes if your program can use them\n",
    "- simply requesting more nodes or cpus will not make things run faster!\n",
    "- only request GPU or bigmem nodes if you need them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring your job's resource requirements\n",
    "```\n",
    "squeue -u netid -l (your jobs)\n",
    "squeue -j jobid -l (particular job)\n",
    "scontrol show job jobid (during job)\n",
    "sacct -j jobid -l (after job finishes)\n",
    "```\n",
    "Note that the default output for squeue and sacct are not idea.  Put in .bashrc:\n",
    "```\n",
    "export SACCT_FORMAT=\"JobID%-20,JobName,User,Partition,NodeList,Elapsed,State,ExitCode,MaxRSS, AllocTRES%32\"\n",
    "export SQUEUE_FORMAT=\"%.16i %.12P %.12j %.8u %.2t %.12M %.12l %24R %.4D %.4C %m %8b %8f\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NOTE run through entire example (sbatch, scontrol, sacct /usr/bin/time -a )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking cpu and memory utilization for running jobs\n",
    "- use ```squeue -u netid``` to determine where your job is running\n",
    "- ssh to those nodes, especially 2nd and on, and run ```top -u netid```\n",
    "- look at %CPU and RES columns\n",
    "- should see ~%100 cpu for each allocated core\n",
    "EXAMPLE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking about memory requirements\n",
    "- default 5GB/allocated cpu on our clusters\n",
    "- strictly enforced; jobs exceeding limit killed\n",
    "- you can request custom memory per node or core with sbatch or srun:\n",
    "```\n",
    "--mem=6g\n",
    "--mem-per-cpu=6g\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining memory requirements for completed jobs\n",
    "\n",
    "1. Before run begins:\n",
    "$ /usr/bin/time -a _prog args_\n",
    "\n",
    "2. After run completes, determine actual usage:\n",
    "$ sacct -j jobid\n",
    "EXAMPLE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slurm sacct\n",
    "\n",
    "sacct -o 'JobID,MaxRSS,MaxVMSize' -j _jobid_\n",
    "\n",
    "or \n",
    "\n",
    "Configure sacct format:\n",
    "\n",
    " export SACCT_FORMAT=JobID%-20,JobName,User,Partition,NodeList,Elapsed,State,ExitCode,MaxRSS, AllocTRES%32\n",
    " \n",
    " sacct -j _jobid_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remora\n",
    "https://github.com/TACC/remora\n",
    "\n",
    "module load REMORA\n",
    "remora prog args ...\n",
    "\n",
    "This will create a directory: remora_jobid\n",
    "\n",
    "Copy (rsync) to local computer, open remora_summary.html with browser \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Compute Resources\n",
    "\n",
    "To get overall sense:\n",
    "```\n",
    "sinfo -p general\n",
    "```\n",
    "To see completely idle nodes, by core count:\n",
    "\n",
    "```\n",
    "$ sinfo -p general -e -t IDLE -o \"%P %.5a %c %.10l %.6D %.6t %N\"\n",
    "PARTITION AVAIL CPUS  TIMELIMIT  NODES  STATE NODELIST\n",
    "general*    up 8 30-00:00:0     35   idle c06n[10-16],c07n[01-14,16],c08n[01-06,08-14]\n",
    "general*    up 16 30-00:00:0     23   idle c10n[13-16],c11n[01-16],c12n[09-11]\n",
    "```\n",
    "\n",
    "Hint, use alias:\n",
    "```\n",
    "alias findidle='sinfo -p general -e -t IDLE -o \"%P %.5a %c %.10l %.6D %.6t %N\"'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairshare scheduling\n",
    "- Groups and users with heavy recent usage (last 30-45 days) have lower priority\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scavenge Partition\n",
    "- Compute nodes in other partions are available via scavenge partition\n",
    "- sbatch -p scavenge ...\n",
    "- separate per user limits apply\n",
    "- works best for short jobs, dSQ/array jobs, or jobs that checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Memory Nodes\n",
    "- We have some compute nodes with 512GB-1.5TB of RAM\n",
    "- Reserved for applications with large memory needs. Please be considerate.\n",
    "- Separate slurm partition: bigmem\n",
    "\n",
    "Typical allocation: \n",
    "```\n",
    "srun/sbatch -p bigmem --mem=1500g ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Nodes\n",
    "- Some applications have been ported to GPUs with impressive performance improvement\n",
    "- Gpu nodes have conventional cpus with multiple cores, and 1-4 GPUs.  \n",
    "- To use GPUs, you must:\n",
    " - request node(s) with GPUs\n",
    " - request the type and number of GPUs \n",
    "\n",
    "Typical allocation:\n",
    "```\n",
    "srun/sbatch -p gpu -c 20 --gres=gpu:1080ti:4 ...\n",
    "```\n",
    "Note that partition names, types and number of GPUs vary by cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism\n",
    "\n",
    "- Sbatch can allocate multiple cores and nodes, but the script runs on one core on one node sequentially.\n",
    "\n",
    "- Simply allocating more nodes or cores DOES NOT make jobs faster.\n",
    "\n",
    "- How do we use multiple cores to increase speed?\n",
    "\n",
    "\n",
    "- Two classes of parallelism:\n",
    " - Lots of independent sequential jobs\n",
    " - Single job parallelized (somehow)\n",
    " \n",
    "\n",
    "- Some options:\n",
    " - Submit many batch jobs simultaneously (not good)\n",
    " - Use job arrays, or dSQ (much better)\n",
    " - Submit a parallel version of your program (great if you have one)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Arrays\n",
    "\n",
    "- Useful when you have many nearly identical, independent jobs to run\n",
    "- Starts many identical copies of your script, distinguished by a task id.\n",
    "\n",
    "Submit jobs like this:\n",
    "```\n",
    "sbatch --array=1-100 ...\n",
    "```\n",
    "Inside your batch script this environment variable to do something different in each task:\n",
    "```\n",
    "./mycommand -i input.${SLURM_ARRAY_TASK_ID} \\\n",
    "    -o output.${SLURM_ARRAY_TASK_ID}\n",
    "```\n",
    "\n",
    "A few nice features of job arrays:\n",
    "- only one job to keep track of\n",
    "- easy to start or cancel entire set\n",
    "- time limits apply to each task, not overall job\n",
    "- your allocation can grow and shrink as conditions change\n",
    "- when using scavenge partition, tasks are killed, but job persists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dSQ (aka Dead Simple Queue)\n",
    "- built on job arrays.  Same nice features, but easier to use\n",
    "- more flexible; tasks can be different from one another\n",
    "- reporting and error recovery built in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using dSQ\n",
    "\n",
    "\n",
    "- Create file containing list of commands to run (jobs.txt)\n",
    "```\n",
    "prog arg1 arg2 -o job1.out\n",
    "prog arg1 arg2 -o job2.out\n",
    "...\n",
    "```\n",
    "- Create launch script\n",
    "```\n",
    "module load dSQ\n",
    "dSQ --taskfile jobs.txt [slurm args] > run.sh\n",
    "```\n",
    "\n",
    "slurm args can specify partion, timelimit, memory, etc. in the usual way.\n",
    "\n",
    "- Submit launch script\n",
    "```\n",
    "sbatch run.sh\n",
    "```\n",
    "\n",
    "For more info, see <http://research.computing.yale.edu/support/hpc/user-guide/dead-simple-queue>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dSQ Reporting\n",
    "- When dSQ job is finished, you'll see a file `job_<jobid>_status.tsv`\n",
    "- Generate report:\n",
    "```\n",
    "$ dSQAutopsy jobs.txt job_<jobid>_status.tsv > failedjobs.txt\n",
    "Autopsy Task Report:\n",
    "9 succeeded\n",
    "1 failed\n",
    "0 didn't run.\n",
    "```\n",
    "\n",
    "- If any jobs failed, failedjobs.txt will contain those jobs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some ways to run in parallel\n",
    "- R: multicore\n",
    "- Python: multiprocessing\n",
    "- C: threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Namd example\n",
    "- molecular dynamics simulation\n",
    "- STMV virus 1,066,628 atoms, 500 time steps\n",
    "\n",
    "\n",
    "![alt text](Examples\\Namd\\namd.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Namd Performance on ? Cluster\n",
    "\n",
    "   |startup (s)|simulate|total|step|step s/u\n",
    "---|-------|--------|-----|----|----\n",
    "1 cpu|26|?|~5400|11.6|1.0\n",
    "20 cpu|64|270|334|0.54|21\n",
    "20 cpu+4 gpu|64|32|96|.064|181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Namd Performance on Grace\n",
    "\n",
    "   |startup (s)|simulate|total|step|step s/u\n",
    "---|-------|--------|-----|----|----\n",
    "1 node, 1 cpu|14|5833|5847|11.6|1.0\n",
    "1 node, 28 cpu|13|218|231|0.44|26.3\n",
    "1 node, 20 cpu+4 K80 gpus|12|32|44|.056|207\n",
    "9 nodes, 40 cpus|29.5|212|242|.424|27.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in /home/fas/lsprog/rdb9/repos/PracticalHPC/Examples/Namd/stmv\n",
    "- gpus slurm-9139008.out\n",
    "- mpi slurm-9152709.out\n",
    "- 28 core MP slurm-9139007.out\n",
    "- 1 core slurm-9140416.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show \n",
    "- namd on 1 cpu\n",
    "- namd on 1 cpu asking for more\n",
    "- namd on several nodes\n",
    "memory required (while running, sacct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Multicore\n",
    "\n",
    "Many R packages have parallelism built in: e.g. bootstrapping (boot)\n",
    "\n",
    "```\n",
    "cores=Sys.getenv(\"SLURM_CPUS_ON_NODE\")\n",
    "boot(data=trees, statistic=volume_estimate, R=50000, parallel=\"multicore\", ncpus=cores)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's actually going on?\n",
    "R uses lapply to apply a function to an array of values.  \n",
    "multicore uses parlapply to do the same thing in parallel with almost no change to code\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
